{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import trange, tqdm\n",
    "from utils import *\n",
    "from modules import *\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8d935f42d0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLayer(torch.nn.Module):\n",
    "    '''\n",
    "    Module implementing a single Bayesian feedforward layer.\n",
    "    The module performs Bayes-by-backprop, that is, mean-field\n",
    "    variational inference. It keeps prior and posterior weights\n",
    "    (and biases) and uses the reparameterization trick for sampling.\n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim, prior_mu=0, prior_sigma=0.1, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.use_bias = bias\n",
    "\n",
    "        self.prior_mu = prior_mu\n",
    "        self.prior_sigma = prior_sigma\n",
    "        self.prior_logsigma = math.log(self.prior_sigma)\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(self.output_dim, self.input_dim))\n",
    "        self.weight_logsigma = nn.Parameter(torch.Tensor(self.output_dim, self.input_dim))\n",
    "        self.register_buffer('weight_eps', None)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias_mu = nn.Parameter(torch.zeros(output_dim))\n",
    "            self.bias_logsigma = nn.Parameter(torch.zeros(output_dim))\n",
    "            self.register_buffer('bias_eps', None)\n",
    "        else:\n",
    "            self.register_parameter('bias_mu', None)\n",
    "            self.register_parameter('bias_logsigma', None)\n",
    "            \n",
    "        self.init_parameters()\n",
    "            \n",
    "    def init_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight_mu.size(1))\n",
    "        self.weight_mu.data.uniform_(-stdv, stdv)\n",
    "        self.weight_logsigma.data.fill_(math.log(self.prior_sigma))\n",
    "        if self.use_bias :\n",
    "            self.bias_mu.data.uniform_(-stdv, stdv)\n",
    "            self.bias_logsigma.data.fill_(math.log(self.prior_sigma))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        weight = self.weight_mu + torch.exp(self.weight_logsigma) * torch.randn_like(self.weight_logsigma)\n",
    "        bias = None\n",
    "        if self.use_bias:\n",
    "            bias = self.bias_mu + torch.exp(self.bias_logsigma) * torch.randn_like(self.bias_logsigma)\n",
    "\n",
    "        return F.linear(inputs, weight, bias)\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        '''\n",
    "        Computes the KL divergence between the priors and posteriors for this layer.\n",
    "        '''\n",
    "        kl_loss = self._kl_divergence(self.weight_mu, self.weight_logsigma)\n",
    "        if self.use_bias:\n",
    "            kl_loss_bias = self._kl_divergence(self.bias_mu, self.bias_logsigma)\n",
    "            kl_loss += kl_loss_bias\n",
    "            kl_loss /= 2\n",
    "\n",
    "        return kl_loss\n",
    "\n",
    "    def _kl_divergence(self, mu, logsigma):\n",
    "        '''\n",
    "        Computes the KL divergence between one Gaussian posterior\n",
    "        and the Gaussian prior.\n",
    "        '''\n",
    "        kl = logsigma - self.prior_logsigma + \\\n",
    "            (math.exp(self.prior_logsigma)**2 + (self.prior_mu - mu)**2) / (2*torch.exp(logsigma)**2) - 0.5\n",
    "\n",
    "        return kl.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesNet(torch.nn.Module):\n",
    "    '''\n",
    "    Module implementing a Bayesian feedforward neural network using\n",
    "    BayesianLayer objects.\n",
    "    '''\n",
    "    def __init__(self, input_size, num_layers, width, prior_mu=0, prior_sigma=0.1, dropout_p=0.5, dropout_at_eval=True):\n",
    "        super().__init__()\n",
    "        self.output_dim = 10\n",
    "        self.dropout_p  = dropout_p\n",
    "        \n",
    "        input_layer = torch.nn.Sequential(BayesianLayer(input_size, width, prior_mu, prior_sigma),\n",
    "                                       nn.ReLU(), nn.Dropout(p=self.dropout_p))\n",
    "        hidden_layers = [nn.Sequential(BayesianLayer(width, width, prior_mu, prior_sigma),\n",
    "                            nn.ReLU(), nn.Dropout(p=self.dropout_p))\n",
    "                             for _ in range(num_layers)\n",
    "                        ]\n",
    "        output_layer = BayesianLayer(width, self.output_dim, prior_mu, prior_sigma)\n",
    "        \n",
    "        layers = [input_layer, *hidden_layers, output_layer]\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "        \n",
    "    def save(self, file_path='bayesnet.pt'):\n",
    "        print(\"Saving BayesNet model to \", file_path)\n",
    "        torch.save(self.net, file_path)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze()\n",
    "        return self.net(x)\n",
    "\n",
    "    def predict_class_probs(self, x, num_forward_passes=10):\n",
    "        x = x.squeeze()\n",
    "        assert x.shape[1] == 28**2\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # TODO: make n random forward passes\n",
    "        # compute the categorical softmax probabilities\n",
    "        # marginalize the probabilities over the n forward passes\n",
    "        probs = x.data.new(num_forward_passes, x.shape[0], self.output_dim)\n",
    "        for i in range(num_forward_passes):\n",
    "            y = self.forward(x)\n",
    "            probs[i] = y\n",
    "        # average over the num_forward_passes dimensions\n",
    "        probs = probs.mean(dim=0, keepdim=False)\n",
    "\n",
    "        assert probs.shape == (batch_size, 10)\n",
    "        return F.softmax(probs, dim=1)\n",
    "\n",
    "    def kl_loss(self, reduction='mean'):\n",
    "        '''\n",
    "        Computes the KL divergence loss for all layers.\n",
    "        '''\n",
    "        # TODO: enter your code here\n",
    "        kl = torch.Tensor([0])\n",
    "        kl_sum = torch.Tensor([0])\n",
    "        n = torch.Tensor([0])\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (BayesianLayer)):\n",
    "                kl = m.kl_divergence()\n",
    "                kl_sum += kl\n",
    "                n += len(m.weight_mu.view(-1))\n",
    "                if m.use_bias:\n",
    "                    n += len(m.bias_mu.view(-1))\n",
    "        if reduction == 'mean':\n",
    "            return kl_sum/n\n",
    "        elif reduction == 'sum':\n",
    "            return kl_sum\n",
    "        else:\n",
    "            raise ValueError(\"Error: {0} is not a valid reduction method\".format(reduction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, optimizer, train_loader, num_epochs=100, pbar_update_interval=100, kl_weight=1):\n",
    "    '''\n",
    "    Updates the model parameters (in place) using the given optimizer object.\n",
    "    Returns `None`.\n",
    "\n",
    "    The progress bar computes the accuracy every `pbar_update_interval`\n",
    "    iterations.\n",
    "    '''\n",
    "    criterion = torch.nn.CrossEntropyLoss() # always used in this assignment\n",
    "\n",
    "    pbar = trange(num_epochs)\n",
    "    for i in pbar:\n",
    "        for k, (batch_x, batch_y) in enumerate(train_loader):\n",
    "            batch_x = batch_x.squeeze()\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch_x)\n",
    "            loss = criterion(y_pred, batch_y)\n",
    "\n",
    "            if type(model) == BayesNet:\n",
    "                # BayesNet implies additional KL-loss.\n",
    "                # TODO: enter your code here\n",
    "                kl_loss = model.kl_loss()\n",
    "                loss = loss + kl_weight * kl_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if k % pbar_update_interval == 0:\n",
    "                acc = (model(batch_x).argmax(axis=1) == batch_y).sum().float()/(len(batch_y))\n",
    "                pbar.set_postfix(loss=loss.item(), acc=acc.item())           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256  # Try playing around with this\n",
    "mnist_rotation_angle = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannick/.local/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create train set\n",
    "dataset_train = load_rotated_mnist()\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size,\n",
    "                                           shuffle=True, drop_last=True)\n",
    "\n",
    "# generate a custom rotated test set\n",
    "transformOpt = transforms.Compose([\n",
    "            transforms.RandomRotation(degrees=(mnist_rotation_angle, -mnist_rotation_angle)),\n",
    "            transforms.ToTensor(),\n",
    "            Img2dTo1d(28**2)\n",
    "])\n",
    "test_set = datasets.MNIST(\n",
    "        root='', train=False, transform=transformOpt, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_set,\n",
    "        batch_size=100,\n",
    "        shuffle=False)\n",
    "\n",
    "train_images, train_labels = next(iter(test_loader))\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.set_title(\"Training image\")\n",
    "ax1.imshow(train_images[0].view(28, 28), cmap=\"gray\")\n",
    "ax2.set_title(\"Test image\")\n",
    "ax2.imshow(test_images[0].view(28, 28), cmap=\"gray\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_mu = 0\n",
    "prior_sigma = 0.1\n",
    "num_layers = 2\n",
    "width = 100\n",
    "num_epochs = 100 # You might want to adjust this\n",
    "print_interval = 200\n",
    "learning_rate = 1e-3  # Try playing around with this\n",
    "extended_evaluation = False  # Set this to True for additional model evaluation\n",
    "kl_weight = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [10:29<00:00,  6.29s/it, acc=0.852, loss=0.397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test data\n",
      "Model type: bayesnet\n",
      "Accuracy = 0.652\n",
      "ECE = 0.107\n"
     ]
    }
   ],
   "source": [
    "private_test = None\n",
    "model_type = \"bayesnet\"  # Try changing this to \"densenet\" as a comparison\n",
    "if model_type == \"bayesnet\":\n",
    "    model = BayesNet(input_size=784, num_layers=num_layers, width=width, prior_mu=prior_mu, prior_sigma=prior_sigma)\n",
    "elif model_type == \"densenet\":\n",
    "    model = Densenet(input_size=784, num_layers=num_layers, width=width)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_network(model, optimizer, train_loader,\n",
    "             num_epochs=num_epochs, pbar_update_interval=print_interval, kl_weight=kl_weight)\n",
    "\n",
    "if test_loader is None:\n",
    "    print(\"evaluating on train data\")\n",
    "    test_loader = train_loader\n",
    "else:\n",
    "    print(\"evaluating on test data\")\n",
    "\n",
    "# Do not change this! The main() method should return the predictions for the test loader\n",
    "predictions = evaluate_model(model, model_type, test_loader, batch_size, extended_evaluation, private_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize prior\n",
    "\n",
    "2x100, wm0s01, kl3: acc = 0.756, ECE = 0.144, score = 1.824\n",
    "\n",
    "2x100, wm0s01, kl1.75: acc = 0.727, ECE = 0.123, score = 1.858\n",
    "\n",
    "### Fixed prior\n",
    "\n",
    "Baseline (2x100, wm0s01, kl2): acc = 0.729, ECE = 0.126, score = 1.85\n",
    "\n",
    "5x150, wm0s013, kl25: acc = 0.706, ECE = 0.130, score = 1.816\n",
    "\n",
    "5x150, wm0s008, kl25: acc = 0.771, ECE = 0.180, score = 1.731\n",
    "\n",
    "5x150, wm0s01, kl25: acc = 0.762, ECE = 0.161, score = 1.779\n",
    "\n",
    "5x150, wm0s012, kl25: acc = 0.722, ECE = 0.147, score = 1.781\n",
    "\n",
    "5x150, wm0s012, kl25: acc = 0.737, ECE = 0.141, score = 1.814\n",
    "\n",
    "5x150, wm0s013, kl25: acc = 0.684, ECE = 0.121, score = 1.821\n",
    "\n",
    "5x150, wm0s0125, kl15: acc = 0.742, ECE = 0.134, score = 1.840\n",
    "\n",
    "6x150, wm0s0125, kl15: acc = 0.730, ECE = 0.129, score = 1.843\n",
    "\n",
    "6x150, wm0s013, kl15: acc = 0.760, ECE = 0.170, score = 1.75\n",
    "\n",
    "7x100, wm0s014, kl15: acc = 0.641, ECE = 0.154, score = 1.679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8719999999999999"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(0.690, 0.106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/bn_2x100_m0s014_kl25.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
