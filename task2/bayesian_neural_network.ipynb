{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import trange, tqdm\n",
    "from utils import *\n",
    "from modules import *\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "JITTER = 1e-6\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLayer(torch.nn.Module):\n",
    "    '''\n",
    "    Module implementing a single Bayesian feedforward layer.\n",
    "    The module performs Bayes-by-backprop, that is, mean-field\n",
    "    variational inference. It keeps prior and posterior weights\n",
    "    (and biases) and uses the reparameterization trick for sampling.\n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim, prior_mu=0, prior_sigma=0.1, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.use_bias = bias\n",
    "\n",
    "        # TODO: enter your code here\n",
    "        self.prior_mu = prior_mu\n",
    "        self.prior_sigma = prior_sigma\n",
    "        self.prior_logsigma = math.log(self.prior_sigma)\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(self.output_dim, self.input_dim))\n",
    "        self.weight_logsigma = nn.Parameter(torch.Tensor(self.output_dim, self.input_dim))\n",
    "        self.register_buffer('weight_eps', None)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias_mu = nn.Parameter(torch.zeros(output_dim))\n",
    "            self.bias_logsigma = nn.Parameter(torch.zeros(output_dim))\n",
    "            self.register_buffer('bias_eps', None)\n",
    "        else:\n",
    "            self.register_parameter('bias_mu', None)\n",
    "            self.register_parameter('bias_logsigma', None)\n",
    "            \n",
    "        self.init_parameters()\n",
    "            \n",
    "    def init_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight_mu.size(1))\n",
    "        stdv = 0.1\n",
    "        self.weight_mu.data.uniform_(-stdv, stdv)\n",
    "        self.weight_logsigma.data.fill_(self.prior_logsigma)\n",
    "        if self.use_bias :\n",
    "            self.bias_mu.data.uniform_(-stdv, stdv)\n",
    "            self.bias_logsigma.data.fill_(self.prior_logsigma)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        weight = self.weight_mu + (torch.exp(self.weight_logsigma) + JITTER) * torch.randn_like(self.weight_logsigma)\n",
    "        bias = None\n",
    "        if self.use_bias:\n",
    "            bias = self.bias_mu + (torch.exp(self.bias_logsigma) + JITTER) * torch.randn_like(self.bias_logsigma)\n",
    "\n",
    "        return F.linear(inputs, weight, bias)\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        '''\n",
    "        Computes the KL divergence between the priors and posteriors for this layer.\n",
    "        '''\n",
    "        kl_loss = self._kl_divergence(self.weight_mu, self.weight_logsigma)\n",
    "        if self.use_bias:\n",
    "            kl_loss_bias = self._kl_divergence(self.bias_mu, self.bias_logsigma)\n",
    "            kl_loss += kl_loss_bias\n",
    "\n",
    "        return kl_loss\n",
    "\n",
    "    def _kl_divergence(self, mu, logsigma):\n",
    "        '''\n",
    "        Computes the KL divergence between one Gaussian posterior\n",
    "        and the Gaussian prior.\n",
    "        '''\n",
    "        # TODO: enter your code here\n",
    "        kl = logsigma - self.prior_logsigma + \\\n",
    "            (math.exp(self.prior_logsigma)**2 + (self.prior_mu - mu)**2) / (2*torch.exp(logsigma)**2) - 0.5\n",
    "\n",
    "        return kl.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesNet(torch.nn.Module):\n",
    "    '''\n",
    "    Module implementing a Bayesian feedforward neural network using\n",
    "    BayesianLayer objects.\n",
    "    '''\n",
    "    def __init__(self, input_size, num_layers, width, prior_mu=0, prior_sigma=0.1,):\n",
    "        super().__init__()\n",
    "        self.output_dim = 10\n",
    "        \n",
    "        if type(width) == list:\n",
    "            input_layer = torch.nn.Sequential(BayesianLayer(input_size, width[0], prior_mu, prior_sigma),\n",
    "                                           nn.ReLU())\n",
    "            hidden_layers = []\n",
    "            for i in range(len(width)-1):\n",
    "                hidden_layers.append(\n",
    "                    torch.nn.Sequential(BayesianLayer(width[i], width[i+1], prior_mu, prior_sigma))\n",
    "                )\n",
    "            output_layer = BayesianLayer(width[-1], self.output_dim, prior_mu, prior_sigma)\n",
    "        else:\n",
    "            input_layer = torch.nn.Sequential(BayesianLayer(input_size, width, prior_mu, prior_sigma),\n",
    "                                           nn.ReLU())\n",
    "            hidden_layers = [nn.Sequential(BayesianLayer(width, width, prior_mu, prior_sigma),\n",
    "                                nn.ReLU()) for _ in range(num_layers)]\n",
    "            output_layer = BayesianLayer(width, self.output_dim, prior_mu, prior_sigma)\n",
    "        \n",
    "        layers = [input_layer, *hidden_layers, output_layer]\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "        \n",
    "    def save(self, file_path='bayesnet.pt'):\n",
    "        print(\"Saving BayesNet model to \", file_path)\n",
    "        torch.save(self.net, file_path)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze()\n",
    "        return self.net(x)\n",
    "\n",
    "    def predict_class_probs(self, x, num_forward_passes=10):\n",
    "        x = x.squeeze()\n",
    "        assert x.shape[1] == 28**2\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # TODO: make n random forward passes\n",
    "        # compute the categorical softmax probabilities\n",
    "        # marginalize the probabilities over the n forward passes\n",
    "        probs = x.data.new(num_forward_passes, x.shape[0], self.output_dim)\n",
    "        for i in range(num_forward_passes):\n",
    "            y = self.forward(x)\n",
    "            probs[i] = y\n",
    "        # average over the num_forward_passes dimensions\n",
    "        probs = probs.mean(dim=0, keepdim=False)\n",
    "\n",
    "        assert probs.shape == (batch_size, 10)\n",
    "        return F.softmax(probs, dim=1)\n",
    "\n",
    "    def kl_loss(self, reduction='mean'):\n",
    "        '''\n",
    "        Computes the KL divergence loss for all layers.\n",
    "        '''\n",
    "        # TODO: enter your code here\n",
    "        kl = torch.Tensor([0])\n",
    "        kl_sum = torch.Tensor([0])\n",
    "        n = torch.Tensor([0])\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (BayesianLayer)):\n",
    "                kl = m.kl_divergence()\n",
    "                kl_sum += kl\n",
    "                n += len(m.weight_mu.view(-1))\n",
    "                if m.use_bias:\n",
    "                    n += len(m.bias_mu.view(-1))\n",
    "        if reduction == 'mean':\n",
    "            return kl_sum/n\n",
    "        elif reduction == 'sum':\n",
    "            return kl_sum\n",
    "        else:\n",
    "            raise ValueError(\"Error: {0} is not a valid reduction method\".format(reduction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, optimizer, train_loader, num_epochs=100, pbar_update_interval=100, kl_weight=1):\n",
    "    '''\n",
    "    Updates the model parameters (in place) using the given optimizer object.\n",
    "    Returns `None`.\n",
    "\n",
    "    The progress bar computes the accuracy every `pbar_update_interval`\n",
    "    iterations.\n",
    "    '''\n",
    "    criterion = torch.nn.CrossEntropyLoss() # always used in this assignment\n",
    "\n",
    "    pbar = trange(num_epochs)\n",
    "    for i in pbar:\n",
    "        for k, (batch_x, batch_y) in enumerate(train_loader):\n",
    "            batch_x = batch_x.squeeze()\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch_x)\n",
    "            loss = criterion(y_pred, batch_y)\n",
    "\n",
    "            if type(model) == BayesNet:\n",
    "                # BayesNet implies additional KL-loss.\n",
    "                # TODO: enter your code here\n",
    "                kl_loss = model.kl_loss()\n",
    "                loss = loss + kl_weight * kl_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if k % pbar_update_interval == 0:\n",
    "                acc = (model(batch_x).argmax(axis=1) == batch_y).sum().float()/(len(batch_y))\n",
    "                pbar.set_postfix(loss=loss.item(), acc=acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256  # Try playing around with this\n",
    "mnist_rotation_angle = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannick/.local/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create train set\n",
    "dataset_train = load_rotated_mnist()\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size,\n",
    "                                           shuffle=True, drop_last=True)\n",
    "\n",
    "# generate a custom rotated test set\n",
    "transformOpt = transforms.Compose([\n",
    "            transforms.RandomRotation(degrees=(mnist_rotation_angle, -mnist_rotation_angle)),\n",
    "            transforms.ToTensor(),\n",
    "            Img2dTo1d(28**2)\n",
    "])\n",
    "test_set = datasets.MNIST(\n",
    "        root='', train=False, transform=transformOpt, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_set,\n",
    "        batch_size=100,\n",
    "        shuffle=False)\n",
    "\n",
    "train_images, train_labels = next(iter(test_loader))\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.set_title(\"Training image\")\n",
    "ax1.imshow(train_images[0].view(28, 28), cmap=\"gray\")\n",
    "ax2.set_title(\"Test image\")\n",
    "ax2.imshow(test_images[0].view(28, 28), cmap=\"gray\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_mu = 0\n",
    "prior_sigma = 0.11\n",
    "num_layers = 3\n",
    "width = 75\n",
    "num_epochs = 150 # You might want to adjust this\n",
    "print_interval = 100\n",
    "learning_rate = 1e-4  # Try playing around with this\n",
    "extended_evaluation = True  # Set this to True for additional model evaluation\n",
    "kl_weight = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2/150 [00:12<15:49,  6.41s/it, acc=0.0781, loss=2.35]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-64fb18ba3d28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m train_network(model, optimizer, train_loader,\n\u001b[0;32m---> 10\u001b[0;31m              num_epochs=num_epochs, pbar_update_interval=print_interval, kl_weight=kl_weight)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-9f52c9db6b9e>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(model, optimizer, train_loader, num_epochs, pbar_update_interval, kl_weight)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0;31m# BayesNet implies additional KL-loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;31m# TODO: enter your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mkl_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkl_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkl_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-30755e875513>\u001b[0m in \u001b[0;36mkl_loss\u001b[0;34m(self, reduction)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBayesianLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mkl_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-47d60959a7a1>\u001b[0m in \u001b[0;36mkl_divergence\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mkl_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kl_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_logsigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mkl_loss_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kl_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_logsigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mkl_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mkl_loss_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-47d60959a7a1>\u001b[0m in \u001b[0;36m_kl_divergence\u001b[0;34m(self, mu, logsigma)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# TODO: enter your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogsigma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_logsigma\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_logsigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_mu\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogsigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "private_test = None\n",
    "model_type = \"bayesnet\"  # Try changing this to \"densenet\" as a comparison\n",
    "if model_type == \"bayesnet\":\n",
    "    model = BayesNet(input_size=784, num_layers=num_layers, width=width, prior_mu=prior_mu, prior_sigma=prior_sigma)\n",
    "elif model_type == \"densenet\":\n",
    "    model = Densenet(input_size=784, num_layers=num_layers, width=width)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_network(model, optimizer, train_loader,\n",
    "             num_epochs=num_epochs, pbar_update_interval=print_interval, kl_weight=kl_weight)\n",
    "\n",
    "if test_loader is None:\n",
    "    print(\"evaluating on train data\")\n",
    "    test_loader = train_loader\n",
    "else:\n",
    "    print(\"evaluating on test data\")\n",
    "\n",
    "# Do not change this! The main() method should return the predictions for the test loader\n",
    "predictions = evaluate_model(model, model_type, test_loader, batch_size, extended_evaluation, private_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline: acc = 0.729, ECE = 0.126, score = 1.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8499999999999999"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(0.728, 0.126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/bn_3x50_m0s01_kl2.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
